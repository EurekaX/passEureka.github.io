<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>仿生人会涮电子羊吗</title>
  
  <subtitle>Then he will be a true love of mine</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2020-11-05T05:36:18.379Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Wendy</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习线性代数基础</title>
    <link href="http://example.com/2020/11/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    <id>http://example.com/2020/11/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</id>
    <published>2020-11-05T04:36:09.000Z</published>
    <updated>2020-11-05T05:36:18.379Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习线性代数基础"><a href="#机器学习线性代数基础" class="headerlink" title="机器学习线性代数基础"></a>机器学习线性代数基础</h1><p>在机器学习和深度学习中，我们涉及到线性代数的这些知识：</p><ul><li><p>向量与矩阵</p></li><li><p>线性方程组</p></li><li><p>向量空间</p></li><li><p>偏差</p></li></ul><p>通过线性代数，我们可以实现以下机器学习或深度学习方法：</p><ul><li><p>推导回归方程</p></li><li><p>通过线性方程预测目标值</p></li><li><p>支持向量机SVM</p></li><li><p>降维</p></li><li><p>均方差或损失函数</p></li><li><p>正则化</p></li><li><p>协方差矩阵</p></li><li><p>卷积</p><h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><p>矩阵是线性代数的重要概念。一个m<em>n矩阵包含mn个元素，可用于线性方程组或线性映射的计算，也可将其视为一个由m</em>n个实值元素组成的元组。</p></li></ul><p><img src="/images/pasted-1.png" alt="upload successful"></p><h2 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h2><p>在线性代数中，向量是大小为n*1的矩阵，即只有一列。</p><h2 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h2><p>矩阵乘法是行和列的点积，其中一个矩阵的行与另一个矩阵列相乘并求和。</p><p><img src="/images/pasted-2.png" alt="upload successful"></p><h3 id="矩阵乘法在线性回归上的运用"><a href="#矩阵乘法在线性回归上的运用" class="headerlink" title="矩阵乘法在线性回归上的运用"></a>矩阵乘法在线性回归上的运用</h3><p>通过多种特征可以预测房屋价格。下表展示了不同房屋的特征及其价格。</p><p><img src="/images/pasted-3.png" alt="不同房屋的特征及其价格"><br>        不同房屋的特征及其价格</p><p><img src="/images/pasted-4.png" alt="upload successful"><br>令：</p><p><img src="/images/pasted-5.png" alt="upload successful"> 特征及其系数<br>可得到房价预测函数</p><p><img src="/images/pasted-6.png" alt="upload successful"></p><h2 id="矩阵转置"><a href="#矩阵转置" class="headerlink" title="矩阵转置"></a>矩阵转置</h2><p>对于矩阵A∈R^m<em>n，有矩阵B∈R^n</em>m满足b_ij = a_ij，称为A的转置，即B=A^T。</p><p><img src="/images/pasted-7.png" alt="upload successful"><br>numpy函数：<code> np.transpose(matrix)</code></p><h2 id="逆矩阵"><a href="#逆矩阵" class="headerlink" title="逆矩阵"></a>逆矩阵</h2><p>对n阶矩阵A，有矩阵B∈R^n*n满足AB =I_n（单位矩阵）= BA的性质，称B为A的逆，表示为A^-1。</p><p><code>np.linalg.inv(matrix)</code></p><p>求矩阵 A 的伪逆（广义逆矩阵）:</p><p><code>np.linalg.pinv(matrix) </code></p><h2 id="正交矩阵"><a href="#正交矩阵" class="headerlink" title="正交矩阵"></a>正交矩阵</h2><p>当且仅当矩阵列向量组是单位正交向量组时，n阶矩阵A∈R^n*n是正交矩阵，有：</p><p><img src="/images/pasted-8.png" alt="upload successful"></p><h2 id="对角矩阵"><a href="#对角矩阵" class="headerlink" title="对角矩阵"></a>对角矩阵</h2><p>在n阶矩阵A∈R^n*n中，除主对角线上的元素，其他所有元素均为零，称其为对角矩阵。</p><h3 id="正规方程的转置矩阵和逆矩阵"><a href="#正规方程的转置矩阵和逆矩阵" class="headerlink" title="正规方程的转置矩阵和逆矩阵"></a>正规方程的转置矩阵和逆矩阵</h3><p><img src="/images/pasted-13.png" alt="upload successful"><br>正规方程通过计算theta j的导数，将其设为零来最小化J。无需Gradient Descent就可直接得到θ的值，θ见下图。</p><p><img src="/images/pasted-10.png" alt="upload successful"></p><p><img src="/images/pasted-11.png" alt="upload successful"><br>创建特征x和目标y的矩阵：<br>import numpy as np Features<br>x = np.array([[2, 1834, 1],[3, 1534, 2],[2, 962, 3]])# Target or Pricey = [8500, 9600, 258800]</p><p>计算x的转置：<br><img src="/images/pasted-12.png" alt="upload successful"><br>计算转置矩阵与原始矩阵乘积的逆：</p><h3 id="线性回归中的线性方程"><a href="#线性回归中的线性方程" class="headerlink" title="线性回归中的线性方程"></a>线性回归中的线性方程</h3><p>回归就是给出线性方程的过程，该过程试图找到满足特定数据集的最优曲线<br>通过线性回归预测平方英尺和房屋价格的关系。<br>数据读取：<br><code>import pandas as pd df = pd.read_csv(&#39;house_price.csv&#39;) df.head()</code></p><p>计算均值、方差、协方差：</p><p><code>def get_mean(value):     total = sum(value)     length = len(value)     mean = total/length     return mean</code><br><code>def get_variance(value):     mean = get_mean(value)     mean_difference_square = [pow((item - mean), 2) for item in value]     variance = sum(mean_difference_square)/float(len(value)-1)     return variance</code></p><p><code>def get_covariance(value1, value2):     value1_mean = get_mean(value1)     value2_mean = get_mean(value2)     values_size = len(value1)     covariance = 0.0    for i in range(0, values_size):         covariance += (value1[i] - value1_mean) * (value2[i] - value2_mean)     return covariance / float(values_size - 1)</code></p><p> 线性回归过程:<br>`<br>def linear_regression(df):<br>   X = df[‘square_feet’]<br>   Y = df[‘price’]<br>   m = len(X)<br>   square_feet_mean = get_mean(X)<br>   price_mean = get_mean(Y)</p><p>   #variance of X<br>   square_feet_variance = get_variance(X)<br>   price_variance = get_variance(Y)</p><p>   covariance_of_price_and_square_feet = get_covariance(X, Y)<br>   w1 = covariance_of_price_and_square_feet / float(square_feet_variance)<br>   w0 = price_mean - w1 * square_feet_mean</p><p>   #prediction –&gt; Linear Equation<br>   prediction = w0 + w1 * X</p><p>   df[‘price (prediction)’] = prediction<br>   return df[‘price (prediction)’]<br>` </p><h2 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h2><p>向量范数可用于衡量向量的大小，也就是说，范数|| x ||表示变量x的大小，范数|| x-y ||表示两个向量x和y之间的距离。</p><p>向量范数计算公式：</p><p><img src="/images/pasted-14.png" alt="upload successful"><br>常用的向量范数为一阶和二阶：</p><p>一阶范数也叫Manhattan范数</p><p>二阶范数也叫Euclidean范数 </p><p>在正则化中会用到一阶和二阶范数。<br>一阶范数/Manhattan范数<br>x∈R^n的L1范数定义为：</p><p><img src="/images/pasted-15.png" alt="upload successful"></p><p>示意图：<br><img src="/images/pasted-16.png" alt="upload successful"></p><p>L2范数/Euclidean范数<br>x∈R^n的L2范数定义为：</p><p><img src="/images/pasted-17.png" alt="upload successful"></p><p><img src="/images/pasted-18.png" alt="upload successful"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;机器学习线性代数基础&quot;&gt;&lt;a href=&quot;#机器学习线性代数基础&quot; class=&quot;headerlink&quot; title=&quot;机器学习线性代数基础&quot;&gt;&lt;/a&gt;机器学习线性代数基础&lt;/h1&gt;&lt;p&gt;在机器学习和深度学习中，我们涉及到线性代数的这些知识：&lt;/p&gt;
&lt;ul&gt;
&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>hello</title>
    <link href="http://example.com/2020/10/29/hello/"/>
    <id>http://example.com/2020/10/29/hello/</id>
    <published>2020-10-29T14:04:31.000Z</published>
    <updated>2020-10-29T14:04:31.178Z</updated>
    
    
    
    
    
  </entry>
  
</feed>
